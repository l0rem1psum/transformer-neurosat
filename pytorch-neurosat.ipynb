{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def repeat_end(val, n, k):\n",
    "    return [val for _ in range(n)] + [k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, d_in, d_outs):\n",
    "        super(MLP, self).__init__()\n",
    "        self.d_in = d_in\n",
    "        self.d_outs = d_outs\n",
    "        \n",
    "        self.linears = torch.nn.ModuleList()\n",
    "        self.activation_func = torch.nn.ReLU()\n",
    "        \n",
    "        self._initialize_layers()\n",
    "        \n",
    "    def _initialize_layers(self):\n",
    "        d_in = self.d_in\n",
    "        for d_out in self.d_outs:\n",
    "            l = torch.nn.Linear(d_in, d_out)\n",
    "            torch.nn.init.xavier_uniform_(l.weight)\n",
    "            torch.nn.init.zeros_(l.bias)\n",
    "            self.linears.append(l)\n",
    "            d_in = d_out\n",
    "    \n",
    "    def forward(self, x):\n",
    "        for i, linear in enumerate(self.linears):\n",
    "            x = linear(x)\n",
    "            if i < len(self.linears) - 1:\n",
    "                x = self.activation_func(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormBasicLSTMCell(torch.nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(LayerNormBasicLSTMCell, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.fiou_linear = torch.nn.Linear(input_size + hidden_size, hidden_size*4, bias=False)\n",
    "        self.fiou_ln_layers = torch.nn.ModuleList(torch.nn.LayerNorm(hidden_size) for _ in range(4))\n",
    "        self.cell_ln = torch.nn.LayerNorm(hidden_size)\n",
    "        \n",
    "    def forward(self, input, state):\n",
    "        hidden_tensor, cell_tensor = state\n",
    "        fiou_linear = self.fiou_linear(torch.cat([input, hidden_tensor], dim=1))\n",
    "        fiou_linear_tensors = fiou_linear.split(self.hidden_size, dim=1)\n",
    "        fiou_linear_tensors = tuple(ln(tensor) for ln, tensor in zip(self.fiou_ln_layers, fiou_linear_tensors))\n",
    "        \n",
    "        f, i, o = tuple(torch.sigmoid(tensor) for tensor in fiou_linear_tensors[:3])\n",
    "        u = torch.tanh(fiou_linear_tensors[3])\n",
    "        \n",
    "        new_cell = self.cell_ln(i * u + f * cell_tensor)\n",
    "        new_h = o * torch.tan(new_cell)\n",
    "        \n",
    "        return new_h, new_cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuroSAT(torch.nn.Module):\n",
    "    def __init__(self, d, n_msg_layers, n_vote_layers, n_rounds):\n",
    "        super(NeuroSAT, self).__init__()\n",
    "        \n",
    "        self.d = d\n",
    "        self.n_rounds = n_rounds\n",
    "        \n",
    "        self.L_init = torch.nn.Parameter(torch.empty([1, d]))\n",
    "        self.C_init = torch.nn.Parameter(torch.empty([1, d]))\n",
    "        \n",
    "        self.LC_msg = MLP(d, repeat_end(d, n_msg_layers, d))\n",
    "        self.CL_msg = MLP(d, repeat_end(d, n_msg_layers, d))\n",
    "        \n",
    "        self.L_update = LayerNormBasicLSTMCell(2*d, d)\n",
    "        self.C_update = LayerNormBasicLSTMCell(d, d)\n",
    "        \n",
    "        self.L_vote = MLP(d, repeat_end(d, n_vote_layers, 1))\n",
    "        self.vote_bias = torch.nn.Parameter(torch.empty([]))\n",
    "        \n",
    "        self._init_weight()\n",
    "        \n",
    "    def _init_weight(self):\n",
    "        torch.nn.init.normal_(self.L_init)\n",
    "        torch.nn.init.normal_(self.C_init)\n",
    "        torch.nn.init.zeros_(self.vote_bias)\n",
    "        \n",
    "    def forward(self, x, n_batches=1):\n",
    "        n_lits, n_clauses = x.size()\n",
    "        n_vars = n_lits // 2\n",
    "        denom = math.sqrt(self.d)\n",
    "        \n",
    "        L_state_h = (self.L_init / denom).repeat([n_lits, 1])\n",
    "        L_state_c = torch.zeros([n_lits, self.d])\n",
    "        \n",
    "        C_state_h = (self.C_init / denom).repeat([n_clauses, 1])\n",
    "        C_state_c = torch.zeros([n_clauses, self.d]) \n",
    "        \n",
    "        for i in range(self.n_rounds):\n",
    "            LC_pre_msgs = self.LC_msg(L_state_h)\n",
    "            LC_msgs = x.t() @ LC_pre_msgs\n",
    "            C_state_h, C_state_c = self.C_update(LC_msgs, (C_state_h, C_state_c))\n",
    "            \n",
    "            CL_pre_msgs = self.CL_msg(C_state_h)\n",
    "            CL_msgs = x @ CL_pre_msgs\n",
    "            xx = torch.cat([L_state_h[n_vars:n_lits, :], L_state_h[0:n_vars, :]], 0)\n",
    "            xxx = torch.cat([CL_msgs, xx], 1)\n",
    "            L_state_h, L_state_c = self.L_update(xxx, (L_state_h, L_state_c))\n",
    "            \n",
    "        all_votes = self.L_vote(L_state_h)\n",
    "        all_votes_join = torch.cat([all_votes[0:n_vars], all_votes[n_vars:n_lits]], 1)\n",
    "        \n",
    "        all_votes_batched = torch.reshape(all_votes_join, [n_batches, n_vars // n_batches, 2])\n",
    "        logits = torch.mean(all_votes_batched, [1,2]) + self.vote_bias\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(logits, labels, parameters):\n",
    "    # x = logits, z = labels\n",
    "    # max(x, 0) - x * z + log(1 + exp(-abs(x)))\n",
    "    predict_costs = torch.nn.functional.relu(logits) - logits * labels + torch.log1p(1 + torch.exp(-torch.abs(logits)))\n",
    "    predict_cost = torch.mean(predict_costs)\n",
    "    l2_cost = torch.zeros([])\n",
    "    for p in parameters:\n",
    "        l2_cost += torch.sum(torch.square(p)) / 2\n",
    "    return predict_cost + 1e-9 * l2_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = NeuroSAT(5, 3, 3, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0.], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n(torch.randn([2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 5])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(1.0986, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_loss(n(torch.randn([2,3])), torch.tensor(1), n.parameters())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
